% ---------- chapters/discussion.tex ----------
\chapter{Discussion and Future Work}\label{ch:discussion}

\section{Lessons Learned}

\begin{enumerate}
    \item \textbf{Streaming the dataset} avoided GPU idling and removed the need to pre‑shuffle the corpus, which is helpful when storage or RAM is limited.
    \item \textbf{Gradient truncation} to the last six recurrences was enough for the model to benefit from looping, yet kept peak memory well under 24 GB.
    \item \textbf{Depth matters.}  Performance improved by two orders of
          magnitude between \(r=1\) and \(r=6\), then deteriorated for
          deeper loops.  Identifying that sweet‑spot is therefore crucial
          for efficient deployment.
    \item \textbf{Lightweight implementation.}  Re‑using the same
          60 M‑parameter core allowed us to explore depth sweeps without
          retraining or touching the checkpoint, making experimentation
          rapid.
\end{enumerate}

\section{Future Directions}

\begin{itemize}
    \item \textbf{Add an early‑exit mechanism}.  
          A simple convergence or KL‑based test could stop unnecessary iterations and further cut inference cost.
    \item \textbf{Token‑recurrence baseline}.  
          Implement and benchmark the token‑level loop proposed in the
          project plan to quantify gains attributable to latent versus
          surface recurrence.
    \item \textbf{Larger data slice}.  
          Train on the full \texttt{OpenMath‑Instruct 1.0} corpus and measure whether the optimal depth shifts with more data.
    \item \textbf{Distillation}.  
          Investigate whether a shallow student (e.g.\ \(r=2\))
          can learn to approximate the behaviour of the deeper teacher.
\end{itemize}
